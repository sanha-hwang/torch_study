{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 제작\n",
    "\n",
    "기본 신경망 학습 절차\n",
    "\n",
    "1. 가중치를 가지고 있는 신경망을 정의 (가중치는 초기화되지 않은 임의의 수, pre_trained라면 기존의 가지고 있는 가중치로 시작)\n",
    "2. data의 입력\n",
    "3. 신경망의 순전파로 data를 계산한  prediction 도출\n",
    "4. 사전 정의된 loss function을 통해 정답과 prediction의 loss를 계산\n",
    "5. 신경망 매개변수에 변화도를 역전파함\n",
    "6. 신경망의 가중치를 사전에 정의한 갱신 규칙(=optimizer)에 따라 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 신경망 정의\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3) ## image 입력 1개, 출력 6개, 커널 사이즈 3x3 정사각형 컨벌루션 행렬\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16*6*6, 120 ) ## 6x6 사이즈 이미지 차원 16를  펼쳐서 120개 출력\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) ## 2x2 사이즈 커널로 conv1 후 relu 결과를 맥스 풀링, 사이즈 축소\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2) ## 역시나 2x2 사이즈 커널, 정수 하나를 주면 nxn정사각형 커널 행렬로 인식\n",
    "        x = x.view(-1, self.num_flat_features(x)) ## num_flat_features 함수로 펼치기\n",
    "        x = F.relu(self.fc1(x)) # full connected layer 통과\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x): ## 펼쳤을 때 차원을 구하는 함수\n",
    "        size = x.size()[1:] # 배치 차원을 제외한 모든 차원\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n",
      "torch.Size([16, 6, 3, 3])\n",
      "torch.Size([120, 576])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([10, 84])\n"
     ]
    }
   ],
   "source": [
    "## 학습 가능한 매개변수 숫자 반환\n",
    "\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size()) # conv1의 가중치\n",
    "print(params[2].size()) # conv2의 가중치\n",
    "print(params[4].size()) # fc1의 가중치\n",
    "print(params[6].size()) # fc2의 가중치\n",
    "print(params[8].size()) # fc3의 가중치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0398,  0.0643,  0.0542, -0.0516, -0.0764,  0.1428, -0.0488,  0.0165,\n",
      "         -0.0198,  0.0341]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## LeNet의 예상 입력 사이즈는 32x32 사이즈를 변경 시켜줘야함\n",
    "\n",
    "input = torch.randn(1, 1, 32,32)\n",
    "output = net(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모든 매개변수 변화도 버퍼를 0으로 초기화, 무작위 변화도로 역전파\n",
    "\n",
    "net.zero_grad()\n",
    "output.backward(torch.randn(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3228, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## 손실함수 정의\n",
    "\n",
    "criterion = nn.MSELoss() ## nn에서 지원하는 MSE 로스 함수\n",
    "\n",
    "# ex \n",
    "\n",
    "target = torch.rand(1,10)\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward0 object at 0x137816e20>\n",
      "<AddmmBackward0 object at 0x1378166a0>\n",
      "<AccumulateGrad object at 0x137816e20>\n"
     ]
    }
   ],
   "source": [
    "## 역전파를 추적\n",
    "\n",
    "print(loss.grad_fn) # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0]) # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad befort backward\n",
      "None\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0056, -0.0007, -0.0081, -0.0237,  0.0012, -0.0122])\n"
     ]
    }
   ],
   "source": [
    "## 역전파를 위해  존재하는 가중치 버퍼를 제거, 제거하지 않으면 변화도 누적\n",
    "## backward를 이미 시행을 하면 가중치를 기록했던 메모리 버퍼가 해제되어, 두번재 backward를 선언하면 runtime error를 발생, 위의 output의 backward를 하지 않음으로\n",
    "## 실행 오류는 피했으나 최선인지는 모르겠음.\n",
    "\n",
    "net.zero_grad() # 모든 매개변수의 변화도 버퍼를 0으로 만들기\n",
    "\n",
    "print('conv1.bias.grad befort backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 가중치 갱신 방법\n",
    "\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optim에서 가중치 갱신 optimizer를 제공\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01) \n",
    "\n",
    "# 학습 과정(training loop)에서\n",
    "optimizer.zero_grad() # 변화도 버퍼 0으로 만들기\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step() # 갱신하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e109f7d8196a6ddeca6f4bd2c070d7ec740711555ec16900918c0063008ddbb6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('newenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
