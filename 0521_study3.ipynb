{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 제작\n",
    "\n",
    "기본 신경망 학습 절차\n",
    "\n",
    "1. 가중치를 가지고 있는 신경망을 정의 (가중치는 초기화되지 않은 임의의 수, pre_trained라면 기존의 가지고 있는 가중치로 시작)\n",
    "2. data의 입력\n",
    "3. 신경망의 순전파로 data를 계산한  prediction 도출\n",
    "4. 사전 정의된 loss function을 통해 정답과 prediction의 loss를 계산\n",
    "5. 신경망 매개변수에 변화도를 역전파함\n",
    "6. 신경망의 가중치를 사전에 정의한 갱신 규칙(=optimizer)에 따라 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 신경망 정의\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3) ## image 입력 1개, 출력 6개, 커널 사이즈 3x3 정사각형 컨벌루션 행렬\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16*6*6, 120 ) ## 6x6 사이즈 이미지 차원 16를  펼쳐서 120개 출력\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) ## 2x2 사이즈 커널로 conv1 후 relu 결과를 맥스 풀링, 사이즈 축소\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2) ## 역시나 2x2 사이즈 커널, 정수 하나를 주면 nxn정사각형 커널 행렬로 인식\n",
    "        x = x.view(-1, self.num_flat_features(x)) ## num_flat_features 함수로 펼치기\n",
    "        x = F.relu(self.fc1(x)) # full connected layer 통과\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x): ## 펼쳤을 때 차원을 구하는 함수\n",
    "        size = x.size()[1:] # 배치 차원을 제외한 모든 차원\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n",
      "torch.Size([16, 6, 3, 3])\n",
      "torch.Size([120, 576])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([10, 84])\n"
     ]
    }
   ],
   "source": [
    "## 학습 가능한 매개변수 숫자 반환\n",
    "\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size()) # conv1의 가중치\n",
    "print(params[2].size()) # conv2의 가중치\n",
    "print(params[4].size()) # fc1의 가중치\n",
    "print(params[6].size()) # fc2의 가중치\n",
    "print(params[8].size()) # fc3의 가중치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0436,  0.1033, -0.0167, -0.0650, -0.0383, -0.1097,  0.0637,  0.0188,\n",
      "         -0.0086, -0.0044]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## LeNet의 예상 입력 사이즈는 32x32 사이즈를 변경 시켜줘야함\n",
    "\n",
    "input = torch.randn(1, 1, 32,32)\n",
    "output = net(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모든 매개변수 변화도 버퍼를 0으로 초기화, 무작위 변화도로 역전파\n",
    "\n",
    "net.zero_grad()\n",
    "output.backward(torch.randn(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3156, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## 손실함수 정의\n",
    "\n",
    "criterion = nn.MSELoss() ## nn에서 지원하는 MSE 로스 함수\n",
    "\n",
    "# ex \n",
    "\n",
    "target = torch.rand(1,10)\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward0 object at 0x12a3f94f0>\n",
      "<AddmmBackward0 object at 0x12a3f98b0>\n",
      "<AccumulateGrad object at 0x12a3f94f0>\n"
     ]
    }
   ],
   "source": [
    "## 역전파를 추적\n",
    "\n",
    "print(loss.grad_fn) # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0]) # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad befort backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/home/torch_study/0521_study3.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/home/torch_study/0521_study3.ipynb#ch0000016?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mconv1.bias.grad befort backward\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/home/torch_study/0521_study3.ipynb#ch0000016?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(net\u001b[39m.\u001b[39mconv1\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mgrad)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/home/torch_study/0521_study3.ipynb#ch0000016?line=7'>8</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/home/torch_study/0521_study3.ipynb#ch0000016?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mconv1.bias.grad after backward\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/home/torch_study/0521_study3.ipynb#ch0000016?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(net\u001b[39m.\u001b[39mconv1\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mgrad)\n",
      "File \u001b[0;32m~/miniforge3/envs/newenv/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/home/miniforge3/envs/newenv/lib/python3.8/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/home/miniforge3/envs/newenv/lib/python3.8/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///Users/home/miniforge3/envs/newenv/lib/python3.8/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///Users/home/miniforge3/envs/newenv/lib/python3.8/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/home/miniforge3/envs/newenv/lib/python3.8/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///Users/home/miniforge3/envs/newenv/lib/python3.8/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///Users/home/miniforge3/envs/newenv/lib/python3.8/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/newenv/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/home/miniforge3/envs/newenv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///Users/home/miniforge3/envs/newenv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/home/miniforge3/envs/newenv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/home/miniforge3/envs/newenv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/home/miniforge3/envs/newenv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///Users/home/miniforge3/envs/newenv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///Users/home/miniforge3/envs/newenv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "## 역전파를 위해  존재하는 가중치 버퍼를 제거, 제거하지 않으면 변화도 누적\n",
    "\n",
    "net.zero_grad() # 모든 매개변수의 변화도 버퍼를 0으로 만들기\n",
    "\n",
    "print('conv1.bias.grad befort backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e109f7d8196a6ddeca6f4bd2c070d7ec740711555ec16900918c0063008ddbb6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('newenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
